{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEW1TRdPXGrF"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "from google.colab import files\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fDXwG2GmrNVf"
      },
      "outputs": [],
      "source": [
        "# Prompt to upload Kaggle credentials to download data from Kaggle\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7DF5Jt2rPnR"
      },
      "outputs": [],
      "source": [
        "# Downloading dataset from Kaggle\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# chmod 600 -> read + write permissions\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!mkdir dog_dataset\n",
        "%cd dog_dataset\n",
        "!kaggle datasets download catherinehorng/dogbreedidfromcomp\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPpCtM9xs19h"
      },
      "outputs": [],
      "source": [
        "# Unzip data and remove zip folder\n",
        "!unzip -q dog_dataset/dogbreedidfromcomp.zip -d dog_dataset\n",
        "!rm dog_dataset/dogbreedidfromcomp.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQRIZx2mtUju"
      },
      "outputs": [],
      "source": [
        "# Add .jpg extension to all ids\n",
        "labels = pd.read_csv('dog_dataset/labels.csv')\n",
        "labels[\"id\"] = labels[\"id\"].apply(lambda x: x + \".jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd0Xmvlgvj2A"
      },
      "outputs": [],
      "source": [
        "# OPTIONAL: Check to see if dataset is balanced or skewed\n",
        "# This is a widely used dataset from Kaggle, it should be good. Can double check just to be sure.\n",
        "\n",
        "sns.set_theme()\n",
        "plt.figure(figsize=(20,6))\n",
        "sns.countplot(data=labels, x='breed')\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Dog Breed Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGZ88x-80cQS"
      },
      "outputs": [],
      "source": [
        "# Create train, validation, and test splits with ~80 10 10 ratio\n",
        "\n",
        "# seed for reproducible results\n",
        "RANDOM_STATE = 42 # Answer to life, the universe, and everything\n",
        "\n",
        "#r_state = random.randint(0, 99)\n",
        "train_df, test_df = train_test_split(labels, test_size=0.1, random_state=RANDOM_STATE)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.11, random_state=RANDOM_STATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUQhABNn2s80"
      },
      "outputs": [],
      "source": [
        "# Make sure the sizes are compact enough, Colab free tier does NOT give me a GPU to analyze images at 4K\n",
        "\n",
        "image_sizes = [Image.open(f'dog_dataset/train/{img_id}').size for img_id in train_df['id']]\n",
        "\n",
        "widths = [size[0] for size in image_sizes]\n",
        "heights = [size[1] for size in image_sizes]\n",
        "\n",
        "w = np.array(widths)\n",
        "h = np.array(heights)\n",
        "\n",
        "figure, axis = plt.subplots(1, 2)\n",
        "\n",
        "axis[0].hist(w)\n",
        "axis[0].set_title('Widths Distribution')\n",
        "\n",
        "axis[1].hist(h)\n",
        "axis[1].set_title('Heights Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BknFAOR75UcE"
      },
      "outputs": [],
      "source": [
        "# Dimensions picked based on width and height distribution from previous cell\n",
        "SIZE = (350, 350)\n",
        "\n",
        "\n",
        "NUM_CLASSES = len(set(labels['breed']))\n",
        "\n",
        "# Initial hyperparamters\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.001\n",
        "DROPOUT_RATE = 0.7\n",
        "EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1MOYOAj_yoS"
      },
      "outputs": [],
      "source": [
        "# Re-scale image values to be between [0, 1]\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Can try rescale=((1./127.5)-1) for a range of [-1, 1] later on?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOkWwB_RAopP"
      },
      "outputs": [],
      "source": [
        "# Data generators\n",
        "train_generator = train_datagen.flow_from_dataframe(train_df, 'dog_dataset/train', 'id', 'breed', target_size=SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
        "test_generator = test_datagen.flow_from_dataframe(test_df, 'dog_dataset/train', 'id', 'breed', target_size=SIZE, batch_size=BATCH_SIZE, class_mode='categorical')\n",
        "val_generator = val_datagen.flow_from_dataframe(val_df, 'dog_dataset/train', 'id', 'breed', target_size=SIZE, batch_size=BATCH_SIZE, class_mode='categorical')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkbSVlKUCEVO"
      },
      "outputs": [],
      "source": [
        "# Base model\n",
        "input_tensor = Input(shape=(SIZE[0], SIZE[1], 3))\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0Ld0L_tCGQ8"
      },
      "outputs": [],
      "source": [
        "# Output layer\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(DROPOUT_RATE)(x)\n",
        "output = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "# Compile model\n",
        "model = Model(inputs=input_tensor, outputs=output)\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-yk_qS2CIs7"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
        "model_checkpoint = ModelCheckpoint('model.keras', monitor='val_loss', save_best_only=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwlkDEdYCKaB"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data=val_generator,\n",
        "                    steps_per_epoch=train_generator.samples//BATCH_SIZE,\n",
        "                    validation_steps=val_generator.samples//BATCH_SIZE,\n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "#  \"Accuracy\"\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# \"Loss\"\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model on test data\n",
        "score = model.evaluate(test_generator)\n",
        "print(\"Test loss:\", score[0])\n",
        "print(\"Test accuracy:\", score[1])"
      ],
      "metadata": {
        "id": "FzTO0ejgNEun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('final_model.keras')"
      ],
      "metadata": {
        "id": "YlDwkH-tz7xV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}